{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsHErn5DgLv0"
      },
      "source": [
        "## Preface\n",
        "\n",
        "### Getting the model\n",
        "\n",
        "LLaMA models are hosted in several places including [llama.com](https://www.llama.com/llama-downloads), [huggingface](https://huggingface.co/meta-llama), or [kaggle](https://www.kaggle.com/organizations/metaresearch/models).\n",
        "\n",
        "Meta currently requires requesting access to the models prior to being able to download them. The form will be accessable based on the web platform you are going to download it from. Responses were very fast in my experience.\n",
        "\n",
        "For this demo, I decided to use huggingface as I found their instructions easiest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38O-Q09kWK4z"
      },
      "source": [
        "# Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJ5GWamS5igO"
      },
      "source": [
        "First we will install the huggingface CLI which helps with logins and authentication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "--ONP7siMiLz",
        "outputId": "a2b14ff3-1728-4d56-ae4a-70cc144d1af6"
      },
      "outputs": [],
      "source": [
        "!pip install -U \"huggingface_hub[cli]\"\n",
        "!pip install transformers\n",
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HM5U18b5s2H"
      },
      "source": [
        "Once installed, we can go ahead and get logged in. I have opted to create an auth token and it is stored in the notebooks secrets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "LriGdDioX61h"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "login(userdata.get('HF_TOKEN'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7qUkQTIna7O"
      },
      "source": [
        "Import a couple libraries that will be used in later scripts, including transformers and pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sbLTaCAMlAvy"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkTAA9a26Ezh"
      },
      "source": [
        "Designate the model we want to use. This will have to be downloaded the first on the first use, but will be available for future queries from that point. I am using the model id of a predefined tokenizer hosted inside a model repo on huggingface, but directory paths also work if the model is already local."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DVi30_3WMjUv",
        "outputId": "49dc538e-9438-4322-96cd-c5d947c2d343"
      },
      "outputs": [],
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "l3_model = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "l3_tokenizer = AutoTokenizer.from_pretrained(l3_model)\n",
        "\n",
        "l3_pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=l3_model,\n",
        "    tokenizer=l3_tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "print(datetime.datetime.now() - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "collapsed": true,
        "id": "bt-EIvif6Lsg",
        "outputId": "1bdd26b7-d1d4-41c8-ea6b-8bb0bffb5804"
      },
      "outputs": [],
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "sequences = l3_pipeline(\n",
        "    'I have tomatoes, basil and cheese at home. What can I cook for dinner?\\n',\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=l3_tokenizer.eos_token_id,\n",
        "    truncation = True,\n",
        "    max_length=400,\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n",
        "\n",
        "print(datetime.datetime.now() - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZjfQLDTlugs"
      },
      "source": [
        "Example with larger CodeLlama model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 775
        },
        "collapsed": true,
        "id": "W9wCn8cqO_C6",
        "outputId": "fbc0051c-bc3c-4525-ebe2-5b79480432ce"
      },
      "outputs": [],
      "source": [
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "cl_model = \"meta-llama/CodeLlama-7b-hf\"\n",
        "#cl_model = \"meta-llama/Llama-2-7b-hf\"\n",
        "cl_tokenizer = AutoTokenizer.from_pretrained(cl_model)\n",
        "\n",
        "cl_pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=cl_model,\n",
        "    tokenizer=cl_tokenizer,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "print(datetime.datetime.now() - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2fcoAe19Rsu"
      },
      "source": [
        "Send input prompt and settings to pipeline. Then check the result. Some models have more intricate prompt formatting options, as with the Instruct models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "collapsed": true,
        "id": "cYwltDVDeQhJ",
        "outputId": "5ac38dd0-dda7-4728-9d4e-c84e2aeb03fd"
      },
      "outputs": [],
      "source": [
        "base_prompt = \"<s>[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_prompt}[/INST]\"\n",
        "\n",
        "input = base_prompt.format(\n",
        "    system_prompt = \"Answer the following programming questions as a knowledgable engineer.\",\n",
        "    user_prompt = \"What casting options are there in c++?\"\n",
        "  )\n",
        "\n",
        "print(input)\n",
        "\n",
        "start = datetime.datetime.now()\n",
        "print(start)\n",
        "\n",
        "sequences = cl_pipeline(\n",
        "    input,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=cl_tokenizer.eos_token_id,\n",
        "    truncation = True,\n",
        "    max_length=400,\n",
        ")\n",
        "\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")\n",
        "\n",
        "print(datetime.datetime.now() - start)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
